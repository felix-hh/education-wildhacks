export const extraContext = `So in the email example, which is what we're gonna run with, we're imagining that say, you know, we're generating a dataset of emails.\n8:53\nFirst, we decide, is it spam or not spam with some probability and then conditional on that label spam or not spam.\n9:02\nWe're going to sample em different words in our vocabulary.\n9:09\nI'm going to figure out how many times that word is going to show up. So maybe, you know, emails that are not spam are about like sports or, you know,\n9:13\ntravel or your family or whatever and spam emails are about, you know, products or, you know, you want to jackpot a million dollars.\n9:23\nThe goal of. Graphical models is to be able to learn some parameters that define these probably distributions,\n9:33\nprobably why and probably have X given why we're going to try and find the parameters that\n9:43\nmaximize the likelihood that those parameters would have generated this particular dataset,\n9:49\nSyria as we're writing down some assumptions. The graphical model encodes our assumptions about how the data came about,\n9:55\nand we're trying to fit parameters that would be the most likely way for this particular dataset to have been generated from this particular model.\n10:02\nWe'll talk a little bit about how you might change this model,\n10:12\nhow you can make different assumptions and how that would influence what you're actually going to do when it comes to learning.\n10:15\nAt prediction time, if I give you an X, I don't tell you what the why is, the prediction task is saying,\n10:22\nwhat's the probability that this X you've given me came from a Y label or a spam label?\n10:30\nWhat's the problem with this? X came from a not spam labeled email.\n10:36\nWhich of those is greater? That's the one I'm going to predict.\n10:40\nIf they're equally likely to say it's 50 50, if one of them is slightly more like the other, maybe it's 60 40.\n10:43\nBut that's going to give us some probability of is this email spam or not spam given the particular data in that e-mail?\n10:50\nThe naive Bayes factorization. Right, so imagine and is huge.\n11:00\nSo it's going to say it's about 10000 words in our vocabulary.\n11:06\nWe want to consider the probability of seeing these 10000 words in some, you know, number of words per account per word.\n11:09\nIf we want to write down this entire probability distribution,\n11:20\nit would require roughly two to the parameters because we have to say, what's the probability of the word da shows up?\n11:24\nThe probability of the word dog shows up, probably word cat shows up,\n11:31\netc. We should consider all possible counts of all possible words times each other.\n11:35\nRight?\n11:43\nSo imagining that words, you know, the seventh word of the email is dependent on the six or the email on the fifth or the email and so and so forth.\n11:43\nWhat the nine base assumption says is that instead, we can write this as a product or each word individually given the label,\n11:53\nso the probability of seeing the word dog in a spam email doesn't influence directly the probability of seeing the word cat in an email.\n12:06\nThis is a naive assumption, we're kind of unreasonable assumption because you would expect that a email that has certain words is more like,\n12:15\nin other words, you know, if you start writing an email about your favorite sports team, other words that every sports team are more likely to appear.\n12:25\nIt doesn't matter just that it's spam or not spam. Right?\n12:34\nGrammar is a thing. Word order is a thing. You know,\n12:38\nthere are words that are related to each other beyond more than just this one\n12:41\nlabel that has a very high level influence on what the topic of the email is,\n12:45\nright? You can imagine having spam emails that are both about movies.\n12:53\nYou have families that are about movies and you have not spam e-mails that about movies, they might share lots of words.\n12:58\nAnd the words that appear in the document have more to do with the fact that they're about movies, perhaps than they are about spam or not spam.\n13:03\nSo when we're thinking about this model, we can write it as such,\n13:14\nbut we can think about the parameters in terms of first, what's the probability of the email being spam?\n13:19\nBefore we look at the actual content of the email,\n13:27\nif we have key labels for your homework and for basically everything you can imagine that K is equal to, it's either spam or not spam, spam, not spam.\n13:31\nAnd so we need key parameters to keep track of or on the order of key parameters to keep track of just this probability by itself.\n13:41\nAnd then for the probability of giving words of a word, given the label, we need to be able to say,\n13:51\nwhat's the probability of seeing say, you know, say X here refers to does the word dog appear in this document?\n13:59\nMaybe there's an 80 percent chance that the word dog appears in not spam and a 40 percent chance it appears in spam emails.\n14:09\nFor each word, we need to parameters or key parameters and we have em such tables because we have words such as K times and primes.\n14:17\nSo in the naive Bayes assumption, allows us to write this as roughly K times or K times timezone plus OK parameters to describe this model,\n14:26\nas opposed to K to the M three the huge savings by assuming that whether or not the word dog shows up doesn't influence around the word shows up,\n14:36\nall that matters is is it spam or not spam? How are you feeling about this, this model, these parameters?\n14:48\nOK. When we're thinking about how to. Fit this model.\n15:01\nAnd so, first of all, say is that some of these equations are going to come from a handout that is in your homework assignment when you,\n15:08\nyou're in your repo is going to be a naive under-par based PDF that's going to have all of the math you need to do your homework.\n15:15\nAnd it's all the more coherently written in terms of following, like line by line.\n15:24\nAnd so one more thing about this.\n15:32\nAnd so in particular, if you're confused about kind of where equations are coming from, ask, but also you can refer back to that document.\n15:35\nWhat the goal of graphical model, maximum likelihood or maximum, a posteriori fitting.\n15:44\nLike trying to use this probabilistic approach to machine learning. Is that we're trying to find the parameters?\n15:54\nThe best printers that are going to maximize the probability of those parameters being correct, given the data that we have.\n16:01\nAnd we will often write this as the probability of the data, given those parameters,\n16:11\nwe want to find the parameters that maximize that quantity that's going to end up looking something like this where we are going to try\n16:18\nto be maximizing the probability of the labels given the parameters and the probability of the data given the labels and the parameters.\n16:26\nSo our parameters are going to define.\n16:34\nIf you think back to these tables are parameters define the probability of y alone and the probability of X given Y.\n16:37\nAnd so we're trying to pick parameters that are going to have the Y is we've\n16:44\nactually seen be as likely as possible given the parameters we have and the X,\n16:50\nas we've seen with the corresponding Y's, are as likely as possible given the parameters we've seen in these tables.\n16:56\nThe probability of theta alone is going to be a prior over what kinds of data as we want to use for your homework,\n17:04\nyou're not going to implement anything with a prior here. We saw this in, say, linear regression.\n17:12\nThis would allow us to kind of apply regularization to our parameters or have a probabilistic model of regularization.\n17:18\nAnd so when we think about how we actually learn things, the process of finding the parameter values.\n17:28\nSo what should this number be? What should this number be? Is just going to be a matter of counting either?\n17:36\nHow many times does the label, why show up and how many times does the Word X Sum J show up in documents with label?\n17:44\nSo it's going to be relatively easy to fit this model. When we have both the axes and the wives together.\n17:57\nHowever, if you recall, before we start talking about graphical models, we talked about expectation maximization.\n18:06\nAnd we are also going to consider what happens if we have examples x but no labels y, how do we learn the counts for this model from unlabeled?\n18:12\nWe're going to use expectation maximization. And so what we can think about what we're going to get to is saying,\n18:28\nsuppose we have probabilities of X and Y, that's what we're trying to kind of find good parameters for.\n18:36\nWe can know we can use our rules of probability to say that we can sum over Y in order to get just the probability of X.\n18:44\nAnd so trying to find the parameters that maximize the probability of X is equivalent to saying,\n18:52\nLet's consider all possible ways for all of our unlabeled data and say,\n18:59\nlet's try to find theta values that are going to maximize that joint probability, given that we only see the axes.\n19:05\nSo it can be very similar in the fully unsupervised cases can be very similar to like Gauci mixture models or K means clustering,\n19:12\nwhere we're trying to say we're trying to find wires,\n19:22\nwhich I simultaneously find parameters that describe how our YS should be connected to our axes and what those wires should be.\n19:25\nHere, if we're thinking about just looking at exes, we're trying to think about all possible,\n19:36\nwhy is that a given document could have either spam or not spam?\n19:42\nWe're going to both try and predict, is it spam or not? And learn the connection between spam emails and the words that appear within the.\n19:46\nIf we had no labels in the entire document, that would be considered fully unsupervised,\n19:59\nwe have no information on what you know a spam email is or not.\n20:06\nWe're just looking for clusters and we're trying to say, you know,\n20:10\nthese emails were like each other and say that all of those emails are spam and those e-mails are like each other.\n20:14\nSo they're all going to be not spam. If we have some labels, we're going to be in-between fully supervised and fully unsupervised.\n20:19\nThat's really called semi supervised. And so in your homework, you are going to implement fully supervised maneuvers and semi supervised namespace.\n20:26\nOK. Let's just keep on here, so when we're thinking about the new phase model,\n20:38\nthe probability of X given Y is the probability of seeing word X or the entire example x the term document X, given that it has some label y.\n20:47\nSo what's the probability of seeing a particular email, the text of a particular email,\n20:58\ngiven that it's spam email or that same text, given that it's not spam?\n21:03\nIf we wanted to write down this entire distribution, if these are just count saying, does this word show up or not, if the label, why is binary?\n21:11\nThis is where we're going to get our tour to the numbers of parameters.\n21:23\nThere's two of them talking between them minus one combinations of all of the XS, and there's two possible limits.\n21:27\nSo we would need to be able to say that what is the probability of seeing, you know,\n21:35\na given sequence of like this string of indicators of whether or not a word shows up conditional for either limb?\n21:41\nSo this is going to be without any independence assumptions. This is going to be intractable.\n21:50\nIt's going to be sure the parameters very, very big table especially is, you know, reasonably large, say a thousand words.\n21:55\nYou're not going to represent us. Conditional on dependencies are what's going to save us and allow niveis to actually work in general.\n22:03\nIf you remember back to last lecture, we talked about conditional independence after reviewing some probabilities and so conditional\n22:13\nindependence says that if we have two variables X and Y that are conditionally independent,\n22:21\ngiven a given z, we can write them as the probability x z z.\n22:28\nOur example here, I don't know if these are a couple of times is do I need an umbrella versus is the ground wet?\n22:35\nThose are not if I keep data on those things. Those are not independent because on days where the ground is what I'm more often you Bella, vice versa.\n22:42\nBut if you tell me that it's raining on that day or not,\n22:54\n`
