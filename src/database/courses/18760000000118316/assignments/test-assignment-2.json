{
  "name": "test-lecture",
  "date": "2023-04-15",
  "transcript": "you know, was there a lot of mutation in, say, New York because we had a lot of strains that were all discovered in New York?\n10:00\nWhat does that tell us about how this disease virus evolves?\n10:06\nHow might that affect?\n10:10\nAnd so a bunch of smaller stuff also happened with COVID trying to figure out kind of, you know, how do we design vaccines in order to be effective?\n10:14\nAnother example which we can do with images is this is not actually how people do image segmentation in practice,\n10:23\nbut it is a cool way of doing it where in the original image there are.\n10:31\nThis image requires a representation that involves 16 million colors based on the RGV set up,\n10:36\nand we can imagine clustering the points in space in the color dimensions.\n10:44\nAnd we can say, let's replace many colors in some space for that region with a single color that is similar to the point in that region.\n10:50\nLet's find a cluster of colors that are all, say, blue or green.\n11:00\nLet's find the kind of middle green color, and let's replace all of those colors with that one.\n11:05\nIn doing so, we can go from 16 million colors to 10 colors while still maintaining most of the detail.\n11:11\nAnd you can still kind of see only three colors. You kind of lose most of that reproduction, but by this is an unsupervised approach.\n11:19\nWe don't know like what we're trying to do per se.\n11:27\nLike, it's not saying we're not explicitly optimizing to keep this be legible to a human.\n11:31\nWe're just saying, let's group colors together that look similar, such that we can reduce the complexity of this image and use it for something.\n11:38\nQuestions about these examples so far, so the general election results.\n11:47\nOK, so if you remember back to lecture one or two,\n11:58\nwe talked about supervised learning and the key difference difference between supervised and\n12:02\nunsupervised learning is that we're going to no longer have labels on some type of function.\n12:08\nRight. So what we're thinking about trying to predict is this a rabbit? We had one features we had, is it around?\n12:16\nAnd so we were told, is there?\n12:22\nFor regression, we had some excellent points we had of why we wanted to find a target function mapping that would go from X to Y.\n12:25\nWhen we had transportation, we had some actions we were supposed to draw a line, but always on one side of the line.\n12:34\nThe accident that.\n12:39\nIn clustering, we're no longer going to have we're not going to know what our actions and we're always we're not going to know what's around it.\n12:42\nWe're not going to have a. Well, we did supervised learning to take on the set up.\n12:47\nWe have access each X is a c d dimensional vector of real numbers.\n12:55\nWe are positing that there exists some target function as to why the data is a paired set of XS and YS,\n13:03\nand we're trying to find a hypothesis that approximate the start function.\n13:12\nBut so up these first two parts don't rely on knowing what we're trying to predict,\n13:16\nbut everything else is very intrinsically linked to what is it that we're trying to do.\n13:23\nEverything we've talked about so far in the past is saying we're imagining that there is some function\n13:27\nthat relates the data to some labels and we want to approximate that function using the data.\n13:32\nWhat can we do if we no longer have lives, we no longer have power inputs,\n13:40\nwe're no longer trying to approximate some function because we don't know if there is a function to approximate.\n13:45\nSo what can we do?\n13:51\nNo longer labels where we do what we can do is kind of fall back to rather than saying we explicitly have labels on, these are actors in these roles.\n13:54\nWe're going to imagine that there are some groupings within the data.\n14:05\nAnd our goal is to have a set up that can find those groupings automatically and justifying them in terms of similarities in the.\n14:09\nSo our goal was rather than saying, given a label, predict that label,\n14:19\nit's going to be figure out what might be a reasonable set of labels for the database.\n14:25\nSo the pattern matching kind of aspect to supervised machine learning is going to reappear in enterprise machine learning.\n14:31\nThe challenges that we are kind of making it harder for ourselves by trying to find patterns that may or may not actually be.\n14:39\nSo if you are back to the movie lenses that we saw at home at one, you had a bunch of data on users who watched movies and read to them.\n14:50\nAnd in general, you know, you could imagine in that setting that you could spend millions of millions of dollars and thousands\n15:01\nof hours to collect lots and lots of ratings such that users actually label all of the movies.\n15:09\nAnd then that will be a very expensive task to do.\n15:16\nAnd so the goal of clustering is to say, how do we discover say that there are movies that are comedies that are movies\n15:22\nthat are horror films without explored exhaustively categorizing this data?\n15:29\nCan we learn the patterns about, say, how people tend to like movies based on just the data of who happens to have seen which movies where?\n15:36\nAnd so the world clustering is to say, given how much data we're going to divide it up into groups somehow,\n15:46\nand the goal is going to be that within a group, say, comedy movies,\n15:53\nmovies that are labeled that we think are comedy movies are going to be more similar to other movies than they are to movies that are not coming.\n15:58\nAnd so the we're not going to have explicit labels, one can say these are all really like comedy movies.\n16:07\nWe're going to say these are movies in Group one,\n16:14\nand we want movies that are assigned to be one to be more similar to each other than movies, either zero or two or three.\n16:17\nIdeally, these clusters will in fact correspond to labels that we might have wanted to collect in the first place.\n16:25\nSo we did discover a cluster for comedy movies that we did discover a cluster of more fraudulent purchases with credit cards,\n16:32\nbut we don't know what we're going to get when we start.\n16:39\nThere isn't any guarantee that our clusters are going to match up to some real world examples.\n16:43\nAnd it's going to be actually quite tricky to measure the quality of our clusters when we do want to compare against.\n16:51\nRailroad labels. But when we're talking about similarities, we're going to go back to two and use distance measures,\n16:59\nso we're going to be able to say that in most cases we're going to have data with an explicit way of measuring.\n17:10\nIs this movie similar to this movie? How by looking at the features that were maybe based on the ratings that I got from watching?\n17:16\nAnd so in unsupervised learning, we're going to start off with saying we have an ax again and data points.\n17:28\nEach one is going to be a vector of real value attributes and figure out how do we define groups out of.\n17:36\nSo let's figure this out by looking at a very silly example. So suppose I got a bunch of ducks.\n17:46\nHow am I divided into two clusters of ducks?\n17:53\nWhat would be maybe the most simple way the questions?\n17:57\nYeah. So we got some real ducks and we got some cartoon ducks. What's a different way when you cluster these?\n18:03\nYeah, nice black exercise, white. What if I want to do three clusters?\n18:13\nWell. I wait three separate clusters.\n18:23\nOK. Someone wearing like a little bit of clothes, I guess, maybe.\n18:29\nWhich is this Donald? As I got choker.\n18:35\nI don't know. Let's suppose hopefully we can split off perhaps a first round to get rid of\n18:39\ngoing to separate out the real ducks that were separated by color among the $30.\n18:48\nRight? But then like you can imagine that say, we learned some clustering and now we have a new dog who is maybe a little more cartoony,\n18:52\nbut a little bit more realistic than their cartoon dogs. Where does this fit in, right?\n19:02\nWe have four clusters. Maybe this new duck is its own cluster. We want some way of saying, first off,\n19:06\nwhere are clusters and then how do new points we haven't seen before or points as the system adopted, which cluster those belong to?\n19:13\nHow do we move our clusters around? These are really kind of questions that sort.\n19:22\nAnd so we might have points that don't quite fit into any of our clusters.\n19:27\nWe want to have some sense of, you know, are they maybe 50 50 between the clusters, 60 40 so on, so forth?\n19:31\nAnd so from a perspective of what we were trying to do when looking at the Ducks, you could imagine, right,\n19:39\nthat we have some future attributes that are, say, the pixels or maybe some other features about them.\n19:49\nBut assuming that, like, is this a cartoon or what is the average pixel density color darkness of the duck, assuming those aren't actual attributes,\n19:55\nwhat we can think of that we're implicitly doing is finding some way of comparing some vector for real,\n20:10\nnot some doctor, some doctor or Daffy Duck and saying, Are these similar or.\n20:16\nAnd in particular, what we're going to say that we're doing is that we are positing some latent variable say,\n20:23\nis this a cartoon from a well-known children's program? And we're going to call that Z.\n20:30\nAnd we're going to say that every image in the space of ducks with every data point has\n20:36\nsome corresponding z that gives it that is kind of assigning it to one of these clusters.\n20:43\nAnd our goal is to come up with what there's these should be such that roughly speaking,\n20:50\nthe distance between points that share a z is going to be small and the distance between points that don't share Z is going to be large.\n20:58\nAnd so the idea here is that what we want to do is both find a mapping from the\n21:08\nactions we observe to some Z's that we're positing exist in the real world.\n21:16\nAnd then we're one to find basically, where should those these? Think about this in two dimensional space.\n21:22\nWe have two separate tasks, one is saying, given a location of a cluster.\n21:28\nWho are the points that belong to that cluster? And then given some points and maybe something about them.\n21:34\nHow do I figure out where these? And important to note, this is kind of the big idea, this is not how we're actually going to implement this.\n21:39\nWe're going to talk with immigration. Any questions about this prospective so far?\n21:54\nBasically, we're inventing labels we're trying to predict.\n22:00\nAnd when you figure out what's a reasonable way of doing that and how can we stop this problem\n22:04\nsuch that we are likely to come up with good labels that actually correspond to some structure?\n22:09\nYeah. Yeah.\n22:22\nSo this is are something that we are kind of imposing on the structure.\n22:30\nSo depending on.\n22:35\nFor example, if we have three clusters, we might have to make the decision ahead of time that we want to have three possible collisions, right?\n22:38\nSo the Zeus can take on zero one two. Or they can only take on zero one.\n22:45\nRight. And so depending on kind of how we structure what things we want to assume about the data, we might get different clusters.\n22:50\nDoes it help at all to go into it with? Possibly.\n23:00\nSo it depends, if you know, kind of what you're looking for or know something about what you're looking for.\n23:07\nYou can use that to guide what you're doing. Often people will start by later taking a perspective on one dimensionality reduction, which like,\n23:14\nI really only want to look at five variables and one hundred variables you might cluster them into,\n23:23\nsay five Rubens and say my new representation is going to be, you know,\n23:31\nwhat's my how likely do I think it is that you're a dark colored doc versus a cartoon dog?\n23:35\nAnd I'm going to throw away the image representation. I'm just going to keep this kind of cluster representation.\n23:40\nAnd so that might be useful if you knew what you want to do is just kind of say, predict something.\n23:45\nBut if you also knew that you had some other information, that's going to be useful. You also have to add that to your representation.\n23:51\nOther questions about set up so far. So we have to.\n24:02\nSo we're going how about today, yes, in the future, you might not have.\n24:09\nHere is Sydney, is this kind of similar to like newspapers, but yes, in some ways you.\n24:16\nYeah, the challenge is going to be for nearest neighbor, as we were saying for a new point.\n24:26\nI'm going to predict that based on the corresponding labels to all points and we're going to those labels.\n24:31\nInstead, I'm saying, is that like a new point is going to be similar to points nearby.\n24:39\nAnd then I'm going to try and use that in a way that's going to be like\n24:47\nrepresenting the structure of what similarity means and how that ties into the.\n24:50\nOK. So suppose we have some data in space.\n25:00"
}
